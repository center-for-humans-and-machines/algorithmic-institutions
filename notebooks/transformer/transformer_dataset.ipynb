{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = \"../..\"\n",
    "data_file = \"experiments/pilot_random1_player_round_slim.csv\"\n",
    "device = \"cpu\"\n",
    "experiment_names = [\"trail_rounds_2\"]\n",
    "# experiment_names = [\"random_1\"]\n",
    "# dataset_folder = \"data/transformer_clone/random_1_v2\"\n",
    "dataset_folder = \"data/transformer_clone/trail_rounds_2_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "from itertools import permutations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from aimanager.transformer.dataset import MyDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decayed_round_feature(df, decay_start, decay_end):\n",
    "    decay_range = decay_end - decay_start\n",
    "    df.loc[df['round_number'] < decay_start, 'decay_feature'] = 1\n",
    "    df.loc[df['round_number'] >= decay_end, 'decay_feature'] = 0\n",
    "    df.loc[(df['round_number'] >= decay_start) & (df['round_number'] < decay_end), 'decay_feature'] = 1 - ((df['round_number'] - decay_start) / decay_range)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(basedir, data_file))\n",
    "\n",
    "df = df[df['experiment_name'].isin(experiment_names)]\n",
    "\n",
    "# Get all unique player ids\n",
    "unique_player_ids = df['player_id'].unique()\n",
    "\n",
    "# Generate all permutations of player ids\n",
    "player_permutations = list(permutations(unique_player_ids))\n",
    "\n",
    "# Create an empty dataframe to store augmented data\n",
    "augmented_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over all permutations\n",
    "for i, perm in enumerate(player_permutations):\n",
    "    \n",
    "    # Create a copy of the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create a new column for permuted player ids\n",
    "    df_copy['new_player_id'] = df_copy['player_id'].apply(lambda x: perm[unique_player_ids.tolist().index(x)])\n",
    "    \n",
    "    # Add a column indicating permutation group\n",
    "    df_copy['perm_group'] = i\n",
    "\n",
    "    # Concatenate the permuted dataframe to the augmented dataframe\n",
    "    augmented_df = pd.concat([augmented_df, df_copy])\n",
    "\n",
    "# Reset the index\n",
    "augmented_df = augmented_df.reset_index(drop=True)\n",
    "\n",
    "# Use the augmented dataframe from now on\n",
    "df = augmented_df\n",
    "\n",
    "# Create dummy variables for 'new_player_id'\n",
    "df = pd.concat([df, pd.get_dummies(df['new_player_id'], prefix='player_id')], axis=1)\n",
    "\n",
    "# Add cyclic encoding for round_number\n",
    "df['round_number_sin'] = np.sin(2*np.pi*df['round_number']/6)\n",
    "df['round_number_cos'] = np.cos(2*np.pi*df['round_number']/6)\n",
    "\n",
    "# Add linearly decaying feature for the first three rounds\n",
    "df = create_decayed_round_feature(df, 0, 3) \n",
    "\n",
    "# scale 'contribution' and 'punishment'\n",
    "df[['contribution_scaled', 'punishment_scaled']] = df[['contribution', 'punishment']] / 20\n",
    "\n",
    "# Now, let's create the events\n",
    "df_contribution = df.copy()\n",
    "df_contribution['event_type'] = 'contribution'\n",
    "df_contribution['event_id'] = 0\n",
    "df_contribution.rename(columns={'contribution': 'event_value_raw', 'contribution_scaled': 'event_value'}, inplace=True)\n",
    "\n",
    "df_punishment = df.copy()\n",
    "df_punishment['event_type'] = 'punishment'\n",
    "df_punishment['event_id'] = 1\n",
    "df_punishment.rename(columns={'punishment': 'event_value_raw', 'punishment_scaled': 'event_value'}, inplace=True)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([df_contribution, df_punishment])\n",
    "\n",
    "# Make sure the data is still sorted correctly after the concatenation\n",
    "df = df.sort_values(['perm_group', 'episode_id', 'new_player_id', 'round_number', 'event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = ['player_id_0', 'player_id_1', 'player_id_2', 'player_id_3',\n",
    "                 'round_number_sin', 'round_number_cos', 'decay_feature', 'event_value', 'event_id']\n",
    "query_columns = ['player_id_0', 'player_id_1', 'player_id_2', 'player_id_3','round_number_sin', 'round_number_cos', 'decay_feature', 'event_id']\n",
    "target_column = 'event_value_raw'\n",
    "\n",
    "unique_eps = df['episode_id'].unique()\n",
    "\n",
    "# Split the perm_groups into train and validation groups\n",
    "train_eps, val_eps = train_test_split(unique_eps, test_size=0.2)\n",
    "\n",
    "# Create training and validation dataframes based on the split perm_groups\n",
    "train_df = df[df['episode_id'].isin(train_eps)]\n",
    "val_df = df[df['episode_id'].isin(val_eps)]\n",
    "\n",
    "# Create your Dataset\n",
    "train_dataset = MyDataset(train_df, input_columns, target_column, query_columns)\n",
    "val_dataset = MyDataset(val_df, input_columns, target_column, query_columns)\n",
    "\n",
    "# Save the datasets\n",
    "\n",
    "folder = os.path.join(basedir, dataset_folder)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "train_file = os.path.join(folder, 'train_dataset.pt')\n",
    "val_file = os.path.join(folder, 'val_dataset.pt')\n",
    "torch.save(train_dataset, train_file)\n",
    "torch.save(val_dataset, val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 98,  80,  78,  96,  56, 103, 104, 117,  61, 111, 102,  68,  91,\n",
       "        63, 124,  43,  87,  51,  76, 101,  45,  75,  79,  77, 123,  95,\n",
       "       112,  62,  44,  93, 116,  60, 120, 110,  49,  46, 119,  57, 121,\n",
       "        48,  99,  65,  58, 106,  67,  41,  53, 113,  54,  92,  66,  64,\n",
       "        40, 125, 115,  72,  69,  84, 105, 118,  94,  71,  82, 108,  97,\n",
       "        83, 107,  55])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 74,  50,  70,  88, 100,  42,  52,  90,  89, 109,  47, 114,  59,\n",
       "        86,  73, 122,  81,  85])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.queries.shape[-1]\n",
    "train_dataset.data.shape[-1]\n",
    "(train_dataset.targets).max().item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1632, 64, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1632, 64, 8])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20,  1, 20,  2,  0,  1],\n",
       "        [20,  0, 20,  0,  4, 10],\n",
       "        [20,  8,  0,  0,  5,  1],\n",
       "        ...,\n",
       "        [ 5,  5,  5, 10, 10,  0],\n",
       "        [12,  0, 12,  0,  9,  0],\n",
       "        [10,  0, 10,  0, 10,  0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.targets[:,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 20.,  1., 20.,  2.,  0.],\n",
       "        [ 0., 20.,  0., 20.,  0.,  4.],\n",
       "        [ 0., 20.,  8.,  0.,  0.,  5.],\n",
       "        ...,\n",
       "        [ 0.,  5.,  5.,  5., 10., 10.],\n",
       "        [ 0., 12.,  0., 12.,  0.,  9.],\n",
       "        [ 0., 10.,  0., 10.,  0., 10.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.data[:,:6,-2] * 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
