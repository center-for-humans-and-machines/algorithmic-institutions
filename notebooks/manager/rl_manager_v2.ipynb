{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c8ea14",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../../data/training/ah_10/data/model.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "output_path = \"../../data/manager/v2/dev/\"\n",
    "model_args = {\n",
    "    \"hidden_size\": 5,\n",
    "    \"add_rnn\": True,\n",
    "    \"add_edge_model\": True,\n",
    "    \"add_global_model\": False,\n",
    "    \"x_encoding\": [\n",
    "        {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "    ],\n",
    "    \"u_encoding\": [\n",
    "        {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "    ],\n",
    "}\n",
    "opt_args = {\"lr\": 0.003}\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "target_update_freq = 20\n",
    "n_episode_steps = 16\n",
    "n_episodes = 100\n",
    "memory_size = 50\n",
    "sample_args = {\"batch_size\": 10}\n",
    "n_test_episodes = 10\n",
    "eval_freq = 10\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "import os\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.manager.graph_memory import GraphMemory\n",
    "from aimanager.manager.evaluation import ManagerEvaluator\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "output_file =  os.path.join(output_path, 'data', \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 5. Avg common good 10.316250801086426. Avg reward -0.3775736391544342\n",
      "Start episode 10. Avg common good 8.549375534057617. Avg reward -0.624423623085022\n",
      "Start episode 15. Avg common good 9.3725004196167. Avg reward -0.5746736526489258\n",
      "Start episode 20. Avg common good 7.019374847412109. Avg reward -0.6526235342025757\n",
      "Start episode 25. Avg common good 8.32437515258789. Avg reward -0.6914736032485962\n",
      "Start episode 30. Avg common good 7.286874294281006. Avg reward -0.7950736284255981\n",
      "Start episode 35. Avg common good 6.062500476837158. Avg reward -0.8258236050605774\n",
      "Start episode 40. Avg common good 6.619999885559082. Avg reward -0.8791235685348511\n",
      "Start episode 45. Avg common good 5.679374694824219. Avg reward -0.9512236714363098\n",
      "Start episode 50. Avg common good 8.194375991821289. Avg reward -0.7852236032485962\n",
      "Start episode 55. Avg common good 9.107500076293945. Avg reward -0.6447736024856567\n",
      "Start episode 60. Avg common good 8.566874504089355. Avg reward -0.7204736471176147\n",
      "Start episode 65. Avg common good 7.429374694824219. Avg reward -0.825373649597168\n",
      "Start episode 70. Avg common good 6.7300004959106445. Avg reward -0.7685736417770386\n",
      "Start episode 75. Avg common good 10.857500076293945. Avg reward -0.602073609828949\n",
      "Start episode 80. Avg common good 9.443124771118164. Avg reward -0.6951236724853516\n",
      "Start episode 85. Avg common good 8.269999504089355. Avg reward -0.7231236696243286\n",
      "Start episode 90. Avg common good 9.839373588562012. Avg reward -0.5911735892295837\n",
      "Start episode 95. Avg common good 9.291250228881836. Avg reward -0.6664236783981323\n"
     ]
    }
   ],
   "source": [
    "device = th.device(device)\n",
    "artifical_humans = AH_MODELS[artificial_humans_model].load(artificial_humans).to(device)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, n_agents=4, n_contributions=21, n_punishments=31, episode_steps=n_episode_steps, device=device)\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=21, n_punishments=31, model_args=model_args, opt_args=opt_args, gamma=gamma, \n",
    "    target_update_freq=target_update_freq, default_values=artifical_humans.default_values, device=device)\n",
    "\n",
    "replay_mem = GraphMemory(n_episodes=memory_size, n_episode_steps=n_episode_steps, n_nodes=4, device=device)\n",
    "recorder = Memory(n_episodes=n_episodes, n_episode_steps=n_episode_steps, device=device)\n",
    "evaluator = ManagerEvaluator(\n",
    "    n_episodes=n_episodes, n_test_episodes=n_test_episodes, eval_freq=eval_freq, \n",
    "    n_episode_steps=n_episode_steps, output_file=output_file)\n",
    "\n",
    "display_freq = n_episodes // 20\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    if (episode > 1) and (episode % display_freq == 0):\n",
    "        avg_common_good = recorder.last(display_freq)['common_good'].mean()\n",
    "        avg_reward = replay_mem.last(display_freq)['reward'].mean()\n",
    "        print(f'Start episode {episode}. Avg common good {avg_common_good}. Avg reward {avg_reward}')\n",
    "\n",
    "    state = env.init_episode()\n",
    "\n",
    "    manager.init_episode(episode)\n",
    "\n",
    "    for step in count():\n",
    "        state_ = {k: v.unsqueeze(0).unsqueeze(-1) for k, v in state.items()}\n",
    "        obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_action = manager.eps_greedy(q_values=q_values, eps=eps)\n",
    "\n",
    "        state = env.punish(selected_action)\n",
    "        recorder.add(**state, episode_step=step)\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        replay_mem.add(\n",
    "            action=selected_action, reward=reward, \n",
    "            obs=obs)\n",
    " \n",
    "        if done:\n",
    "            replay_mem.next_episode(episode)\n",
    "            \n",
    "            # allow manager to update itself\n",
    "            sample = replay_mem.sample(**sample_args)\n",
    "            \n",
    "\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break\n",
    "    recorder.add(episode_step=step, **state)\n",
    "    recorder.next_episode(episode)\n",
    "    evaluator.eval_episode(manager, env, episode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager_v2.ipynb",
   "output_path": "notebooks/manager/rl_manager_v2.ipynb",
   "parameters": {
    "artificial_humans": "../../data/training/ah_10/data/model.pt",
    "artificial_humans_model": "graph",
    "device": "cuda",
    "eps": 0.2,
    "eval_freq": 10,
    "gamma": 1,
    "memory_size": 50,
    "model_args": {
     "add_edge_model": true,
     "add_global_model": false,
     "add_rnn": true,
     "hidden_size": 5,
     "u_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 16,
       "name": "round_number"
      },
      {
       "etype": "float",
       "name": "prev_common_good",
       "norm": 128
      }
     ],
     "x_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 21,
       "name": "prev_contributions"
      },
      {
       "encoding": "numeric",
       "n_levels": 31,
       "name": "prev_punishments"
      }
     ]
    },
    "n_episode_steps": 16,
    "n_episodes": 100,
    "n_test_episodes": 10,
    "opt_args": {
     "lr": 0.003
    },
    "output_path": "../../data/manager/v2/dev/",
    "sample_args": {
     "batch_size": 10
    },
    "target_update_freq": 20
   },
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "83484b78e3eced0c1ebbaf37dd8049c2f9102f6dcade2a60a08a368fc0daac5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
