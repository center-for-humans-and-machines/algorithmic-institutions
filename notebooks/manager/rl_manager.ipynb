{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa008829",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = (\n",
    "    \"artifacts/artificial_humans/04_3_2_model/model/architecture_node+edge+rnn.pt\"\n",
    ")\n",
    "artificial_humans_valid = \"artifacts/artificial_humans/02_4_valid/model/rnn_True.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "manager_args = {\n",
    "    \"opt_args\": {\"lr\": 0.0003},\n",
    "    \"gamma\": 1.0,\n",
    "    \"eps\": 0.1,\n",
    "    \"target_update_freq\": 100,\n",
    "    \"model_args\": {\n",
    "        \"hidden_size\": 20,\n",
    "        \"add_rnn\": True,\n",
    "        \"add_edge_model\": False,\n",
    "        \"add_global_model\": False,\n",
    "        \"x_encoding\": [\n",
    "            {\"name\": \"contribution\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "            {\"name\": \"prev_punishment\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "        ],\n",
    "        \"b_encoding\": [{\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"onehot\"}],\n",
    "    },\n",
    "}\n",
    "replay_memory_args = {\"n_episodes\": 10}\n",
    "n_update_steps = 20\n",
    "training_batch_size = 3\n",
    "eval_period = 5\n",
    "env_args = {\n",
    "    \"n_agents\": 4,\n",
    "    \"n_contributions\": 21,\n",
    "    \"n_punishments\": 31,\n",
    "    \"n_rounds\": 16,\n",
    "    \"batch_size\": 1000,\n",
    "}\n",
    "device = \"cpu\"\n",
    "seed = 42\n",
    "output_dir = \"../../notebooks/manager/rl_manager/01_rnn_node\"\n",
    "basedir = \"../..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata_dir\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mlocals\u001b[39m():\n\u001b[1;32m     20\u001b[0m     output_dir \u001b[39m=\u001b[39m data_dir\n\u001b[0;32m---> 22\u001b[0m metrics_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39m'\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m model_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m make_dir(metrics_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.utils.utils import make_dir\n",
    "from aimanager.utils.array_to_df import add_labels\n",
    "\n",
    "\n",
    "if \"data_dir\" in locals():\n",
    "    output_dir = data_dir\n",
    "\n",
    "metrics_dir = os.path.join(output_dir, 'metrics')\n",
    "model_dir = os.path.join(output_dir, 'model')\n",
    "make_dir(metrics_dir)\n",
    "make_dir(model_dir)\n",
    "\n",
    "\n",
    "th.random.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# check if job_id is already set\n",
    "if \"job_id\" not in locals():\n",
    "    job_id = 'none'\n",
    "if \"labels\" not in locals():\n",
    "    labels = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9400cf5",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_keys = [\n",
    "    \"punishment\",\n",
    "    \"contribution\",\n",
    "    \"common_good\",\n",
    "    \"contributor_payoff\",\n",
    "    \"manager_payoff\",\n",
    "]\n",
    "\n",
    "\n",
    "replay_keys = [n[\"name\"] for n in manager_args[\"model_args\"][\"x_encoding\"]]\n",
    "replay_keys += [n[\"name\"] for n in manager_args[\"model_args\"][\"b_encoding\"]]\n",
    "replay_keys += [\"punishment\"]\n",
    "replay_keys = list(set(replay_keys))\n",
    "\n",
    "\n",
    "def run_batch(manager, env, replay_mem=None, on_policy=True, update_step=None):\n",
    "\n",
    "    state = env.reset()\n",
    "    metric_list = []\n",
    "    for round_number in count():\n",
    "        # encoded = manager.encode(state)\n",
    "        statecopy = {k: v.clone() for k, v in state.items() if k in replay_keys}\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(\n",
    "            state, first=round_number == 0, edge_index=env.batch_edge_index\n",
    "        )\n",
    "        if on_policy:\n",
    "            action = q_values.argmax(-1)\n",
    "        else:\n",
    "            # Sample a action\n",
    "            action = manager.eps_greedy(q_values=q_values)\n",
    "\n",
    "        state = env.punish(action)\n",
    "\n",
    "        metrics = {k: state[k].to(th.float).mean().item() for k in rec_keys}\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        if replay_mem is not None:\n",
    "            replay_mem.add(\n",
    "                episode_step=round_number, action=action, reward=reward, **statecopy\n",
    "            )\n",
    "\n",
    "        metrics[\"next_reward\"] = reward.mean().item()\n",
    "        metrics[\"q_min\"] = q_values.min().item()\n",
    "        metrics[\"q_max\"] = q_values.max().item()\n",
    "        metrics[\"q_mean\"] = q_values.mean().item()\n",
    "        metrics[\"round_number\"] = round_number\n",
    "        metrics[\"sampling\"] = \"greedy\" if on_policy else \"eps-greedy\"\n",
    "        metrics[\"update_step\"] = update_step\n",
    "        metric_list.append(metrics)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return metric_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mpib/brinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:200: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m manager\u001b[39m.\u001b[39msave(model_file)\n\u001b[1;32m     45\u001b[0m \u001b[39m# test successful storage\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m manager\u001b[39m.\u001b[39;49mload(model_file)\n",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "device = th.device(device)\n",
    "cpu = th.device(\"cpu\")\n",
    "\n",
    "artificial_humans_ = os.path.join(basedir, artificial_humans)\n",
    "artificial_humans_valid_ = os.path.join(basedir, artificial_humans_valid)\n",
    "\n",
    "ah = (\n",
    "    AH_MODELS[artificial_humans_model]\n",
    "    .load(artificial_humans_, device=device)\n",
    "    .to(device)\n",
    ")\n",
    "ahv = (\n",
    "    AH_MODELS[artificial_humans_model]\n",
    "    .load(artificial_humans_valid_, device=device)\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=ah, artifical_humans_valid=ahv, device=device, **env_args\n",
    ")\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=env.n_contributions,\n",
    "    n_punishments=env.n_punishments,\n",
    "    default_values=ah.default_values,\n",
    "    device=device,\n",
    "    **manager_args,\n",
    ")\n",
    "\n",
    "replay_mem = Memory(n_episode_steps=env.n_rounds, device=cpu, **replay_memory_args)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for update_step in range(n_update_steps):\n",
    "    # replay_mem.start_batch(env.groups)\n",
    "\n",
    "    # here we sample one batch of episodes and add them to the replay buffer\n",
    "    off_policy_metrics = run_batch(\n",
    "        manager, env, replay_mem, on_policy=False, update_step=update_step\n",
    "    )\n",
    "\n",
    "    replay_mem.next_episode(update_step)\n",
    "\n",
    "    # allow manager to update itself\n",
    "    sample = replay_mem.get_random(device=device, n_episodes=training_batch_size)\n",
    "\n",
    "    if sample is not None:\n",
    "        loss = manager.update(\n",
    "            update_step, **sample, batch=env.batch, edge_index=env.batch_edge_index\n",
    "        )\n",
    "\n",
    "    if (update_step % eval_period) == 0:\n",
    "        metrics_list.extend(\n",
    "            [{**m, \"loss\": l.item()} for m, l in zip(off_policy_metrics, loss)]\n",
    "        )\n",
    "        metrics_list.extend(\n",
    "            run_batch(\n",
    "                manager, env, replay_mem=None, on_policy=True, update_step=update_step\n",
    "            )\n",
    "        )\n",
    "\n",
    "model_file = os.path.join(model_dir, f\"{job_id}_manager.pt\")\n",
    "\n",
    "manager.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828f50a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<aimanager.manager.manager.ArtificalManager at 0x7fe0993cec70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model saving and loading\n",
    "manager.load(model_file, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6155e",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_vars = [\"round_number\", \"sampling\", \"update_step\"]\n",
    "value_vars = [\n",
    "    \"punishment\",\n",
    "    \"contribution\",\n",
    "    \"common_good\",\n",
    "    \"contributor_payoff\",\n",
    "    \"manager_payoff\",\n",
    "    \"next_reward\",\n",
    "    \"q_min\",\n",
    "    \"q_max\",\n",
    "    \"q_mean\",\n",
    "    \"loss\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame.from_records(metrics_list)\n",
    "\n",
    "df = df.melt(id_vars=id_vars, value_vars=value_vars, var_name=\"metric\")\n",
    "\n",
    "df = add_labels(df, {**labels, \"job_id\": job_id})\n",
    "\n",
    "df.to_parquet(os.path.join(metrics_dir, f\"{job_id}.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager.ipynb",
   "output_path": "notebooks/manager/rl_manager.ipynb",
   "parameters": {
    "artificial_humans": "artifacts/artificial_humans/04_3_2_model/model/architecture_node+edge+rnn.pt",
    "artificial_humans_model": "graph",
    "artificial_humans_valid": "artifacts/artificial_humans/02_4_valid/model/rnn_True.pt",
    "basedir": "../..",
    "device": "cpu",
    "env_args": {
     "batch_size": 1000,
     "n_agents": 4,
     "n_contributions": 21,
     "n_punishments": 31,
     "n_rounds": 16
    },
    "eval_period": 5,
    "manager_args": {
     "eps": 0.1,
     "gamma": 1,
     "model_args": {
      "add_edge_model": false,
      "add_global_model": false,
      "add_rnn": true,
      "b_encoding": [
       {
        "encoding": "onehot",
        "n_levels": 16,
        "name": "round_number"
       }
      ],
      "hidden_size": 20,
      "x_encoding": [
       {
        "encoding": "numeric",
        "n_levels": 21,
        "name": "contribution"
       },
       {
        "encoding": "numeric",
        "n_levels": 31,
        "name": "prev_punishment"
       }
      ]
     },
     "opt_args": {
      "lr": 0.0003
     },
     "target_update_freq": 100
    },
    "n_update_steps": 20,
    "output_dir": "../../notebooks/manager/rl_manager/01_rnn_node",
    "replay_memory_args": {
     "n_episodes": 10
    },
    "seed": 42,
    "training_batch_size": 3
   },
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
