{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialHumanEnv():\n",
    "    \"\"\"\n",
    "    Environment that runs the virtual humans and calculuates the value of the common good.\n",
    "\n",
    "    Indices:\n",
    "        t: agent types [0..1]\n",
    "    \"\"\"\n",
    "    state_dimensions = {\n",
    "        'punishments': ['agent'],\n",
    "        'contributions': ['agent'],\n",
    "        'payoffs': ['agent'],\n",
    "        'valid': ['agent'],\n",
    "        'common_good': ['agent'],\n",
    "        'episode_step': ['agent'],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self, *, contributors_model, n_agents, max_contribution, max_punishment, episode_steps, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            asdasd\n",
    "        \"\"\"\n",
    "        self.episode = 0\n",
    "        self.episode_steps = episode_steps\n",
    "        self.device = device\n",
    "        self.max_contribution = max_contribution\n",
    "        self.max_punishment = max_punishment\n",
    "        self.contributors_model = contributors_model\n",
    "        self.n_agents = n_agents\n",
    "        self.reset_state()\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = {\n",
    "            'punishments': th.zeros(self.n_agents, dtype=th.int16),\n",
    "            'contributions': th.zeros(self.n_agents, dtype=th.int16),\n",
    "            'payoffs': th.zeros(self.n_agents, dtype=th.float32),\n",
    "            'valid': th.zeros(self.n_agents, dtype=th.bool),\n",
    "            'common_good': th.tensor(0, dtype=th.float32),\n",
    "            'episode_step': th.tensor(0, dtype=th.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if 'state' in self.__dict__:\n",
    "            state = self.__dict__['state']\n",
    "            return state[name]\n",
    "\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if 'state' in self.__dict__:\n",
    "            if name in self.__dict__['state']:\n",
    "                self.state[name] = value\n",
    "            else:\n",
    "                object.__setattr__(self, name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_common_good(contributions, punishments):\n",
    "        return contributions.sum() * 1.6 + punishments.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_payout(contributions, punishments, commond_good):\n",
    "        # TODO: check how to handle missing values\n",
    "        return 20 - contributions - punishments + 0.25 * commond_good\n",
    "\n",
    "    def get_contributions(self):\n",
    "        contributions = self.contributors_model.act(**self.state)\n",
    "        return contributions\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.episode += 1\n",
    "        self.episode_step = 0\n",
    "        self.reset_state()\n",
    "        self.contributions = self.get_contributions()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, punishments):\n",
    "        self.episode_step += 1\n",
    "\n",
    "        assert punishments.max() <= self.max_punishment\n",
    "        assert punishments.dtype == th.int64\n",
    "\n",
    "        if (self.episode_step == self.episode_steps):\n",
    "            done = True\n",
    "        elif self.episode_step > self.episode_steps:\n",
    "            raise ValueError('Environment is done already.')\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.punishments = punishments\n",
    "        self.common_good = self.calc_common_good(self.contributions, self.punishments)\n",
    "        self.payoffs = self.calc_payout(self.contributions, self.punishments, self.common_good)\n",
    "        self.contributions = self.get_contributions()\n",
    "\n",
    "        return self.state, self.common_good, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, device, n_episodes, n_episode_steps, output_file):\n",
    "        self.memory = None\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_episode_steps = n_episode_steps\n",
    "        self.device = device\n",
    "        self.output_file = output_file\n",
    "        self.current_row = 0\n",
    "        self.episode_queue = collections.deque([], maxlen=self.n_episodes)\n",
    "\n",
    "\n",
    "    def init_store(self, state):\n",
    "        self.memory = {\n",
    "            th.empty((self.n_episodes, self.n_episode_steps, *t.shape), dtype=t.dtype, device=self.device)\n",
    "            for k, t in state\n",
    "        } + {\n",
    "            'episode': th.empty((self.n_episodes, self.n_episode_steps), dtype=th.int64, device=self.device),\n",
    "            'episode_steps': th.empty((self.n_episodes, self.n_episode_steps), dtype=th.int64, device=self.device)\n",
    "        }\n",
    "\n",
    "    def next_episode(self, episode):\n",
    "        self.current_row = (self.current_row + 1) % self.n_episodes\n",
    "        self.episode = episode\n",
    "        self.episode_queue.appendleft(self.current_row)\n",
    "\n",
    "    def add_state(self, state, episode_step):\n",
    "        self.memory['episode'][self.current_row,episode_step] = self.episode\n",
    "        self.memory['episode_steps'][self.current_row,episode_step] = episode_step\n",
    "        for k, t in state.items():\n",
    "            self.memory[k][self.current_row,episode_step] = t\n",
    "\n",
    "    def sample(self, batch_size, horizon, **kwargs):\n",
    "        eff_horizon = min(len(self), horizon)\n",
    "        relative_episode = np.random.choice(eff_horizon, batch_size, replace=False)\n",
    "        return self.get_relative(relative_episode, **kwargs)\n",
    "\n",
    "    def last(self, batch_size, **kwargs):\n",
    "        assert batch_size <= self.n_episodes\n",
    "        relative_episodes = np.arange(batch_size)\n",
    "        return self.get_relative(relative_episodes, **kwargs)\n",
    "\n",
    "    def get_relative(self, relative_episode, keys=None):\n",
    "        if keys is None:\n",
    "            keys = self.memory.keys()\n",
    "        hist_idx = th.tensor(\n",
    "            [self.episode_queue[rp] for rp in relative_episode], dtype=th.int64, device=self.device)\n",
    "        return {k: v[hist_idx] for k, v in self.memory.items() if k in keys}\n",
    "\n",
    "    def rec(self, state, episode, episode_steps):\n",
    "        if self.memory is None:\n",
    "            self.init_store(state)\n",
    "        self.add_state(state, episode, episode_steps)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.episode_queue)\n",
    "\n",
    "    def write(self):\n",
    "        th.save(\n",
    "            {\n",
    "                k: t[:self.current_row] for k, t in self.memory.items()\n",
    "            },\n",
    "            f'{self.output_file}_{self.episode}.pt'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aimanager.model.neural.random import RandomArtificialHumans\n",
    "\n",
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "rah = RandomArtificialHumans(device=device, max_contribution=20)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    contributors_model=rah, n_agents=4, max_contribution=20, max_punishment=30, episode_steps=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'punishments': tensor([0, 0, 0, 0], dtype=torch.int16), 'contributions': tensor([ 6,  5, 19,  8]), 'payoffs': tensor([0., 0., 0., 0.]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(0.), 'episode_step': tensor(0, dtype=torch.int16)}\n",
      "{'punishments': tensor([ 8,  7, 27,  8]), 'contributions': tensor([ 7, 14,  1, 19]), 'payoffs': tensor([33.7000, 35.7000,  1.7000, 31.7000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(110.8000), 'episode_step': tensor(1, dtype=torch.int16)} tensor(110.8000) False\n",
      "{'punishments': tensor([28,  8,  7,  0]), 'contributions': tensor([15, 18,  2, 16]), 'payoffs': tensor([12.1500, 25.1500, 39.1500, 28.1500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(108.6000), 'episode_step': tensor(2, dtype=torch.int16)} tensor(108.6000) False\n",
      "{'punishments': tensor([10, 27, 30, 10]), 'contributions': tensor([18,  6, 14, 17]), 'payoffs': tensor([34.6500, 14.6500, 27.6500, 33.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(158.6000), 'episode_step': tensor(3, dtype=torch.int16)} tensor(158.6000) False\n",
      "{'punishments': tensor([ 8,  7, 20, 12]), 'contributions': tensor([15, 13,  0,  0]), 'payoffs': tensor([27.7500, 40.7500, 19.7500, 24.7500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(135.), 'episode_step': tensor(4, dtype=torch.int16)} tensor(135.) False\n",
      "{'punishments': tensor([15, 18,  9, 14]), 'contributions': tensor([ 5, 19, 11, 18]), 'payoffs': tensor([15.2000, 14.2000, 36.2000, 31.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(100.8000), 'episode_step': tensor(5, dtype=torch.int16)} tensor(100.8000) False\n",
      "{'punishments': tensor([ 7, 16, 24, 25]), 'contributions': tensor([ 7,  1, 14, 15]), 'payoffs': tensor([47.2000, 24.2000, 24.2000, 16.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(156.8000), 'episode_step': tensor(6, dtype=torch.int16)} tensor(156.8000) False\n",
      "{'punishments': tensor([27, 18, 19, 13]), 'contributions': tensor([ 0, 13,  8, 17]), 'payoffs': tensor([20.0500, 35.0500, 21.0500, 26.0500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(136.2000), 'episode_step': tensor(7, dtype=torch.int16)} tensor(136.2000) False\n",
      "{'punishments': tensor([15, 26, 28, 11]), 'contributions': tensor([4, 5, 3, 2]), 'payoffs': tensor([40.2000, 16.2000, 19.2000, 27.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(140.8000), 'episode_step': tensor(8, dtype=torch.int16)} tensor(140.8000) False\n",
      "{'punishments': tensor([10,  9,  7,  4]), 'contributions': tensor([ 5,  9, 19, 10]), 'payoffs': tensor([19.1000, 19.1000, 23.1000, 27.1000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(52.4000), 'episode_step': tensor(9, dtype=torch.int16)} tensor(52.4000) False\n",
      "{'punishments': tensor([10,  0, 10,  0]), 'contributions': tensor([1, 7, 9, 7]), 'payoffs': tensor([27.2000, 33.2000, 13.2000, 32.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(88.8000), 'episode_step': tensor(10, dtype=torch.int16)} tensor(88.8000) False\n",
      "{'punishments': tensor([ 9, 16,  4, 14]), 'contributions': tensor([ 4, 16, 14, 14]), 'payoffs': tensor([30.3500, 17.3500, 27.3500, 19.3500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(81.4000), 'episode_step': tensor(11, dtype=torch.int16)} tensor(81.4000) False\n",
      "{'punishments': tensor([26, 25, 21, 10]), 'contributions': tensor([16, 19, 11, 11]), 'payoffs': tensor([29.7000, 18.7000, 24.7000, 35.7000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(158.8000), 'episode_step': tensor(12, dtype=torch.int16)} tensor(158.8000) False\n",
      "{'punishments': tensor([11, 11, 14, 28]), 'contributions': tensor([ 8,  2, 12, 14]), 'payoffs': tensor([31.8000, 28.8000, 33.8000, 19.8000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(155.2000), 'episode_step': tensor(13, dtype=torch.int16)} tensor(155.2000) False\n",
      "{'punishments': tensor([ 3, 26, 12,  4]), 'contributions': tensor([17, 12, 17,  8]), 'payoffs': tensor([34.6500, 17.6500, 21.6500, 27.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(102.6000), 'episode_step': tensor(14, dtype=torch.int16)} tensor(102.6000) False\n",
      "{'punishments': tensor([ 0, 18,  0, 24]), 'contributions': tensor([ 4, 16, 18,  8]), 'payoffs': tensor([35.1000, 22.1000, 35.1000, 20.1000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(128.4000), 'episode_step': tensor(15, dtype=torch.int16)} tensor(128.4000) False\n",
      "{'punishments': tensor([ 5, 19,  8, 21]), 'contributions': tensor([14, 13,  9,  9]), 'payoffs': tensor([42.6500, 16.6500, 25.6500, 22.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(126.6000), 'episode_step': tensor(16, dtype=torch.int16)} tensor(126.6000) True\n"
     ]
    }
   ],
   "source": [
    "state = env.init_episode()\n",
    "print(state)\n",
    "done = False\n",
    "while not done:\n",
    "    punishments = th.randint(0, 31, (4,), device=device)\n",
    "    state, reward, done = env.step(punishments)\n",
    "    print(state, reward, done)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
