{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5afbc74d",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "x_encoding = [\n",
    "    {\"encoding\": \"ordinal\", \"column\": \"prev_contribution\"},\n",
    "    {\"encoding\": \"ordinal\", \"column\": \"prev_punishment\"},\n",
    "]\n",
    "u_encoding = [\n",
    "    {\"encoding\": \"numeric\", \"column\": \"prev_common_good\"}\n",
    "]\n",
    "y_encoding = [\n",
    "    {\"encoding\": \"ordinal\", \"column\": \"contribution\"}\n",
    "]\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 2\n",
    "fraction_training = 1.0\n",
    "data = \"../data/pilot1_player_round_slim.csv\"\n",
    "output_path = \"../data/dev\"\n",
    "labels = {}\n",
    "model_args = {\"n_units\": 40}\n",
    "optimizer_args = {\"lr\": 0.0001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 100, \"batch_size\": 40, \"clamp_grad\": 1, \"eval_period\": 10}\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:42.924460Z",
     "iopub.status.busy": "2022-02-15T16:13:42.923968Z",
     "iopub.status.idle": "2022-02-15T16:13:43.653779Z",
     "shell.execute_reply": "2022-02-15T16:13:43.651937Z"
    },
    "papermill": {
     "duration": 0.743457,
     "end_time": "2022-02-15T16:13:43.657200",
     "exception": false,
     "start_time": "2022-02-15T16:13:42.913743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from aimanager.artificial_humans.cross_validation import split_xy, get_cross_validations, get_fraction_of_groups\n",
    "from aimanager.generic.encoder import ordinal_to_int, onehot_to_int, joined_encoder, int_encode\n",
    "from aimanager.artificial_humans.metrics import create_metrics, create_confusion_matrix\n",
    "from aimanager.artificial_humans.synthesize_data import syn_con_pun\n",
    "from aimanager.utils.array_to_df import add_labels, using_multiindex\n",
    "from aimanager.utils.utils import make_dir\n",
    "from aimanager.generic.mlp import MultiLayer\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5aa83ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found a maximum of 32 data points per episode.\n",
      "Remove incomplete episodes. 84 of 3092 data points are going to be removed.\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "th_device = th.device(device)\n",
    "\n",
    "def create_fully_connected(n_nodes):\n",
    "    return th.tensor([[i,j]\n",
    "        for i in range(n_nodes)\n",
    "        for j in range(n_nodes)\n",
    "    ])\n",
    "n_rounds = 8\n",
    "n_player = 4\n",
    "\n",
    "\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "df['contribution'] = pd.Categorical(\n",
    "    df['contribution'], categories=np.arange(n_contributions), ordered=True\n",
    ")\n",
    "df['punishment'] = pd.Categorical(\n",
    "    df['punishment'], categories=np.arange(n_punishments), ordered=True\n",
    ")\n",
    "\n",
    "index = ['session', 'global_group_id', 'episode', 'participant_code', 'round_number']\n",
    "\n",
    "eposide_data_count = df.groupby(by=index[0:3])['contribution'].transform('count')\n",
    "max_eposide_data_count = eposide_data_count.max()\n",
    "w = eposide_data_count != max_eposide_data_count\n",
    "print(f'We found a maximum of {max_eposide_data_count} data points per episode.')\n",
    "print(f'Remove incomplete episodes. {w.sum()} of {w.count()} data points are going to be removed.')\n",
    "df = df[~w]\n",
    "\n",
    "df = df.set_index(index).sort_index()\n",
    "prev_df = df.groupby(index[:-1]).shift(1)\n",
    "rename = {\n",
    "    c: 'prev_' + c\n",
    "    for c in prev_df.columns\n",
    "}\n",
    "prev_df = prev_df.rename(columns=rename)\n",
    "df = df.join(prev_df)\n",
    "df = df[~df['prev_contribution'].isnull()]\n",
    "\n",
    "\n",
    "n_rounds = n_rounds - 1\n",
    "n_episodes = 2\n",
    "n_episodes_total = len(df) // (n_player*n_rounds)\n",
    "n_groups = n_episodes_total // n_episodes\n",
    "\n",
    "x = th.tensor(joined_encoder(df, x_encoding), dtype=th.float, device=th_device).reshape(n_episodes_total,n_rounds,n_player,-1)\n",
    "u = th.tensor(joined_encoder(df, u_encoding), dtype=th.float, device=th_device).reshape(n_episodes_total,n_rounds,n_player,-1)[:,:,0,:]\n",
    "y = th.tensor(joined_encoder(df, y_encoding), dtype=th.float, device=th_device).reshape(n_episodes_total,n_rounds,n_player,-1)\n",
    "edge_attr = th.zeros(n_player*n_player,0)\n",
    "edge_index = create_fully_connected(n_player)\n",
    "group_idx = [\n",
    "    g\n",
    "    for g in range(n_groups)\n",
    "    for e in range(n_episodes)\n",
    "]\n",
    "\n",
    "dataset = [\n",
    "    Data(x=x[i],u=u[i],y=y[i], edge_attr=edge_attr, edge_index=edge_index, idx=i, group_idx=group_idx[i])\n",
    "    for i in range(n_episodes_total)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "091eef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([83])\n",
      "tensor([24])\n",
      "tensor([65])\n",
      "tensor([56])\n",
      "tensor([50])\n",
      "tensor([60])\n",
      "tensor([20])\n",
      "tensor([1])\n",
      "tensor([40])\n",
      "tensor([23])\n",
      "tensor([90])\n",
      "tensor([6])\n",
      "tensor([9])\n",
      "tensor([77])\n",
      "tensor([25])\n",
      "tensor([13])\n",
      "tensor([18])\n",
      "tensor([76])\n",
      "tensor([62])\n",
      "tensor([59])\n",
      "tensor([57])\n",
      "tensor([41])\n",
      "tensor([51])\n",
      "tensor([93])\n",
      "tensor([30])\n",
      "tensor([84])\n",
      "tensor([71])\n",
      "tensor([38])\n",
      "tensor([37])\n",
      "tensor([91])\n",
      "tensor([33])\n",
      "tensor([64])\n",
      "tensor([46])\n",
      "tensor([49])\n",
      "tensor([32])\n",
      "tensor([0])\n",
      "tensor([44])\n",
      "tensor([78])\n",
      "tensor([69])\n",
      "tensor([81])\n",
      "tensor([89])\n",
      "tensor([68])\n",
      "tensor([73])\n",
      "tensor([26])\n",
      "tensor([43])\n",
      "tensor([75])\n",
      "tensor([72])\n",
      "tensor([58])\n",
      "tensor([53])\n",
      "tensor([28])\n",
      "tensor([79])\n",
      "tensor([35])\n",
      "tensor([80])\n",
      "tensor([47])\n",
      "tensor([70])\n",
      "tensor([34])\n",
      "tensor([63])\n",
      "tensor([15])\n",
      "tensor([2])\n",
      "tensor([11])\n",
      "tensor([22])\n",
      "tensor([39])\n",
      "tensor([54])\n",
      "tensor([82])\n",
      "tensor([4])\n",
      "tensor([61])\n",
      "tensor([10])\n",
      "tensor([21])\n",
      "tensor([86])\n",
      "tensor([8])\n",
      "tensor([12])\n",
      "tensor([67])\n",
      "tensor([5])\n",
      "tensor([92])\n",
      "tensor([14])\n",
      "tensor([42])\n",
      "tensor([17])\n",
      "tensor([16])\n",
      "tensor([52])\n",
      "tensor([45])\n",
      "tensor([19])\n",
      "tensor([36])\n",
      "tensor([31])\n",
      "tensor([29])\n",
      "tensor([7])\n",
      "tensor([48])\n",
      "tensor([27])\n",
      "tensor([3])\n",
      "tensor([87])\n",
      "tensor([85])\n",
      "tensor([66])\n",
      "tensor([55])\n",
      "tensor([74])\n",
      "tensor([88])\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "for i,d in enumerate(iter(DataLoader(dataset, shuffle=True))):\n",
    "    print(d.idx)\n",
    "    if i > 10000:\n",
    "        break\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08fc0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_cross_validations(dataset, n_splits):\n",
    "    group_ids = list(set(d.group_idx for d in dataset))\n",
    "    random.shuffle(group_ids)\n",
    "    groups = [group_ids[i::n_splits] for i in range(n_splits)]\n",
    "\n",
    "    for i in range(n_splits):\n",
    "        test_groups = groups[i]\n",
    "        test_dataset = [d for d in dataset if d.group_idx in test_groups]\n",
    "        train_dataset = [d for d in dataset if not d.group_idx in test_groups]\n",
    "        yield train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd8c7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential as Seq, Linear as Lin, Tanh\n",
    "import torch as th\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "class EdgeModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = 2*x_features+edge_features+u_features\n",
    "        self.edge_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        # src, dest: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = th.cat([src, dest, edge_attr, u[batch]], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = x_features+edge_features+u_features\n",
    "        self.node_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = scatter_mean(edge_attr, col, dim=0, dim_size=x.size(0))\n",
    "        out = th.cat([x, out, u[batch]], dim=1)\n",
    "        return self.node_mlp(out)\n",
    "\n",
    "class GlobalModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = u_features+x_features\n",
    "        self.global_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        out = th.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "        return self.global_mlp(out)\n",
    "\n",
    "class GraphNetwork(th.nn.Module):\n",
    "    def __init__(\n",
    "            self, x_features, edge_features, u_features, n_rounds, n_units, out_features, y_encoding):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.y_encoding = y_encoding\n",
    "        self.op1 = MetaLayer(\n",
    "            EdgeModel(\n",
    "                x_features=x_features, edge_features=edge_features, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds), \n",
    "            NodeModel(\n",
    "                x_features=x_features, edge_features=n_units, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds), \n",
    "            GlobalModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds)\n",
    "        )\n",
    "        self.op2 = MetaLayer(\n",
    "            EdgeModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=n_units, out_features=n_units, n_rounds=n_rounds), \n",
    "            NodeModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=n_units, out_features=out_features, n_rounds=n_rounds), \n",
    "            None\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        x, edge_attr, u = self.op1(x, edge_index, edge_attr, u, batch)\n",
    "        x, edge_attr, u = self.op2(x, edge_index, edge_attr, u, batch)\n",
    "        return x\n",
    "\n",
    "    def _pred_and_proba(self, x, edge_index, edge_attr, u, batch):\n",
    "        out = self(x, edge_index, edge_attr, u, batch)\n",
    "        if self.encoding == 'ordinal':\n",
    "            y_pred_proba = th.sigmoid(out).detach().cpu().numpy()\n",
    "            y_pred = ordinal_to_int(y_pred_proba)\n",
    "            y_pred_proba = np.concatenate([np.ones_like(y_pred_proba[:,[0]]), y_pred_proba[:,:]], axis=1)\n",
    "        elif self.encoding == 'onehot':\n",
    "            y_pred_proba = th.nn.functional.softmax(out, dim=-1).detach().cpu().numpy()\n",
    "            y_pred = onehot_to_int(y_pred_proba)\n",
    "        elif self.encoding == 'numeric':\n",
    "            y_pred = th.sigmoid(out).detach().cpu().numpy()\n",
    "            y_pred = np.around(y_pred*self.out_features, decimals=0).astype(np.int64)\n",
    "            y_pred_proba = None\n",
    "        return y_pred, y_pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98777b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dataset(model, dataset, batch_size):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        y_true.append(batch.y)\n",
    "        _y_pred, _y_pred_proba = model._pred_and_proba(\n",
    "            x=batch.x, edge_attr=batch.edge_attr, edge_index=batch.edge_index, u=batch.u)\n",
    "        y_pred_proba.append(_y_pred_proba)\n",
    "        y_pred.append(_y_pred)\n",
    "    y_pred = th.cat(y_pred)\n",
    "    y_true = th.cat(y_true)\n",
    "    y_pred_proba = th.cat(y_pred_proba)\n",
    "    return y_true, y_pred, y_pred_proba\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, batch_size):\n",
    "        self.metrics = []\n",
    "        self.confusion_matrix = []\n",
    "        # self.synthetic_predicitions = []\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def set_data(self, datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def set_labels(self, **labels):\n",
    "        self.labels = labels\n",
    "\n",
    "    def eval_set(self, model, set_name):\n",
    "        model.eval()\n",
    "        y_true, y_pred, y_pred_proba = predict_dataset(model, self.datasets[set_name], self.batch_size)\n",
    "        self.metrics += create_metrics(y_true, y_pred, set=set_name, **self.labels)\n",
    "        self.confusion_matrix += create_confusion_matrix(y_true, y_pred, set=set_name, **self.labels)\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.metrics.append(dict(name='loss', value=loss, **self.labels))\n",
    "\n",
    "    def save(self, output_path, labels):\n",
    "        make_dir(output_path)\n",
    "        self._save_metric(self.metrics, 'metrics.parquet', output_path, labels)\n",
    "        self._save_metric(self.confusion_matrix, 'confusion_matrix.parquet', output_path, labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_metric(rec, filename, output_path, labels):\n",
    "        df = pd.DataFrame(rec)\n",
    "        df = add_labels(df, labels)\n",
    "        df.to_parquet(os.path.join(output_path, filename))\n",
    "\n",
    "\n",
    "class Loss(th.nn.Module):\n",
    "    def __init__(self, encoding, n_contributions):\n",
    "        super().__init__()\n",
    "        self.encoding = encoding\n",
    "        self.n_contributions = n_contributions\n",
    "        if encoding == 'ordinal':\n",
    "            self.loss_fn = th.nn.BCEWithLogitsLoss()\n",
    "        elif encoding == 'onehot':\n",
    "            self.loss_fn = th.nn.CrossEntropyLoss()\n",
    "        elif encoding == 'numeric':\n",
    "            self.loss_fn = th.nn.MSELoss()\n",
    "        else:\n",
    "            raise NotImplementedError(f'Unkown target encoding: {encoding}')\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if self.encoding == 'numeric':\n",
    "            input = th.sigmoid(input)*self.n_contributions\n",
    "        return self.loss_fn(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.354450Z",
     "iopub.status.busy": "2022-02-15T16:13:44.354021Z",
     "iopub.status.idle": "2022-02-15T16:14:11.080737Z",
     "shell.execute_reply": "2022-02-15T16:14:11.080147Z"
    },
    "papermill": {
     "duration": 26.739528,
     "end_time": "2022-02-15T16:14:11.084012",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.344484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/MPIB-BERLIN/brinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12313/3508246426.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'batch'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "batch_size = train_args['batch_size']\n",
    "x_features = dataset[0].x.shape[-1]\n",
    "edge_features = dataset[0].edge_attr.shape[-1]\n",
    "u_features = dataset[0].u.shape[-1]\n",
    "out_features = dataset[0].y.shape[-1]\n",
    "n_rounds = dataset[0].x.shape[-2]\n",
    "\n",
    "ev = Evaluator(batch_size=batch_size)\n",
    "\n",
    "for i, (train_dataset, test_dataset) in enumerate(get_cross_validations(dataset, n_cross_val)):\n",
    "    model = GraphNetwork(\n",
    "        x_features=x_features, edge_features=edge_features, \n",
    "        u_features=u_features, n_rounds=n_rounds, \n",
    "        out_features=out_features, y_encoding=y_encoding[0]['encoding'],\n",
    "        **model_args).to(th_device)\n",
    "    loss_fn = Loss(y_encoding[0]['encoding'], n_contributions)\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "\n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "    for e in range(n_episodes):\n",
    "        loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for batch in loader:\n",
    "            py = model(x=batch.x, edge_attr=batch.edge_attr, edge_index=batch.edge_index, u=batch.u)\n",
    "            loss = loss_fn(py, batch.y)\n",
    "            loss.backward()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "            \n",
    "            if e % train_args['eval_period'] == 0:\n",
    "                avg_loss = sum_loss/n_steps\n",
    "                print(f'CV {i} | Epoch {e} | Loss {avg_loss}')\n",
    "                ev.add_loss(avg_loss)\n",
    "                ev.eval_set(model, 'train')\n",
    "                ev.eval_set(model, 'test')\n",
    "                sum_loss = 0\n",
    "                n_steps = 0\n",
    "\n",
    "    # ev.eval_sync(model)\n",
    "\n",
    "ev.save(output_path, labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.003052,
   "end_time": "2022-02-15T16:14:11.718838",
   "environment_variables": {},
   "exception": null,
   "input_path": "neural.ipynb",
   "output_path": "neural.ipynb",
   "parameters": {
    "data": "../data/pilot1_player_round_slim.csv",
    "device": "cpu",
    "fraction_training": 1,
    "labels": {},
    "model_args": {
     "hidden_size": 40,
     "n_layers": 2
    },
    "n_contributions": 21,
    "n_cross_val": 2,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 0.00001
    },
    "output_path": "../data/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": 1,
     "epochs": 100,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "column": "prev_contribution",
      "encoding": "ordinal"
     },
     {
      "column": "prev_punishment",
      "encoding": "ordinal"
     }
    ],
    "y_encoding": "numeric"
   },
   "start_time": "2022-02-15T16:13:41.715786",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
